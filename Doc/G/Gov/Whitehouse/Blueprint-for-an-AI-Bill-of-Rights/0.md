Note=BLUEPRINT FOR AN AI BILL OF RIGHTS

Note=MAKING AUTOMATED SYSTEMS WORK FOR THE AMERICAN PEOPLE

Note=OCTOBER 2022

Note=About this Document

Note=The Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People was published by the White House Office of Science and Technology Policy in October 2022. This framework was released one year after OSTP announced the launch of a process to develop “a bill of rights for an AI-powered world.” Its release follows a year of public engagement to inform this initiative. The framework is available online at: https://www.whitehouse.gov/ostp/ai-bill-of-rights

CodersNote=The Glossary of defined terms is presented as a side table. Click on "Document" to see the full text. 

LawyersNote=The Defined terms are expressed with some formal inconsistency. The term "equity" is defined using the (preferred) format of: “Equity” means the ....  Other terms are defined using the (less preferred) format of:  An “automated system” ... or "Communities" include ....  Or Data are sensitive if ....  And “Passive computing infrastructure” is ... embedded in the definition of “Automated system”.  Note also that defining terms in uncapitalized format creates ambiguities about whether variations are intended to be covered by the definition.  E.g., "equitably." 


Ti=BLUEPRINT FOR AN AI BILL OF RIGHTS

1.Ti=SAFE AND EFFECTIVE SYSTEMS

1.sec=You should be protected from unsafe or ineffective systems. Automated systems should be developed with consultation from diverse {_communities}, stakeholders, and domain experts to identify concerns, risks, and potential impacts of the system. Systems should undergo pre-deployment testing, risk identification and mitigation, and ongoing monitoring that demonstrate they are safe and effective based on their intended use, mitigation of unsafe outcomes  including those beyond the intended use, and adherence to domain-specific standards. Outcomes of these protective measures should include the possibility of not deploying the system or removing a system from use. {_Automated_systems} should not be designed with an intent or reasonably foreseeable possibility of endangering your safety or the safety of your {_community}. They should be designed to proactively protect you from harms stemming from unintended, yet foreseeable, uses or impacts of {_automated_systems}. You should be protected from inappropriate or irrelevant data use in the design, development, and deployment of {_automated_systems}, and from the compounded harm of its reuse. Independent evaluation and reporting that confirms that the system is safe and effective, including reporting of steps taken to mitigate potential harms, should be performed and the results made public whenever possible.

2.Ti=ALGORITHMIC DISCRIMINATION PROTECTIONS

2.sec=You should not face discrimination by algorithms and systems should be used and designed in an {_equitable} way. Algorithmic discrimination occurs when {_automated_systems} contribute to unjustified different treatment or impacts disfavoring people based on their race, color, ethnicity, sex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual orientation), religion, age, national origin, disability, veteran status, genetic information, or any other classification protected by law. Depending on the specific circumstances, such algorithmic discrimination may violate legal protections. Designers, developers, and deployers of {_automated_systems} should take proactive and continuous measures to protect individuals and {_communities} from algorithmic discrimination and to use and design systems in an {_equitable} way. This protection should include proactive {_equity} assessments as part of the system design, use of representative data and protection against proxies for demographic features, ensuring accessibility for people with disabilities in design and development, pre-deployment and ongoing disparity testing and mitigation, and clear organizational oversight. Independent evaluation and plain language reporting in the form of an algorithmic impact assessment, including disparity testing results and mitigation information, should be performed and made public whenever possible to confirm these protections.

3.Ti=DATA PRIVACY

3.sec=You should be protected from abusive data practices via built-in protections and you should have agency over how data about you is used. You should be protected from violations of privacy through design choices that ensure such protections are included by default, including ensuring that data collection conforms to reasonable expectations and that only data strictly necessary for the specific context is collected. Designers, developers, and deployers of {_automated_systems} should seek your permission and respect your decisions regarding collection, use, access, transfer, and deletion of your data in appropriate ways and to the greatest extent possible; where not possible, alternative privacy by design safeguards should be used. Systems should not employ user experience and design decisions that obfuscate user choice or burden users with defaults that are privacy invasive. Consent should only be used to justify collection of data in cases where it can be appropriately and meaningfully given. Any consent requests should be brief, be understandable in plain language, and give you agency over data collection and the specific context of use; current hard-to-understand notice-and-choice practices for broad uses of data should be changed. Enhanced protections and restrictions for data and inferences related to {_sensitive_domains}, including health, work, education, criminal justice, and finance, and for data pertaining to youth should put you first. In {_sensitive_domains}, your data and related inferences should only be used for necessary functions, and you should be protected by ethical review and use prohibitions. You and your {_communities} should be free from unchecked surveillance; {_surveillance_technologies} should be subject to heightened oversight that includes at least pre-deployment assessment of their potential harms and scope limits to protect privacy and civil liberties. Continuous surveillance and monitoring should not be used in education, work, housing, or in other contexts where the use of such {_surveillance_technologies} is likely to limit {_rights,_opportunities,_or_access}. Whenever possible, you should have access to reporting that confirms your data decisions have been respected and provides an assessment of the potential impact of {_surveillance_technologies} on your {_rights,_opportunities,_or_access}.

4.Ti=NOTICE AND EXPLANATION

4.sec=You should know that an {_automated_system} is being used and understand how and why it contributes to outcomes that impact you. Designers, developers, and deployers of {_automated_systems} should provide generally accessible plain language documentation including clear descriptions of the overall system functioning and the role automation plays, notice that such systems are in use, the individual or organization responsible for the system, and explanations of outcomes that are clear, timely, and accessible. Such notice should be kept up-to-date and people impacted by the system should be notified of significant use case or key functionality changes. You should know how and why an outcome impacting you was determined by an {_automated_system}, including when the {_automated_system} is not the sole input determining the outcome. Automated systems should provide explanations that are technically valid, meaningful and useful to you and to any operators or others who need to understand the system, and calibrated to the level of risk based on the context. Reporting that includes summary information about these {_automated_systems} in plain language and assessments of the clarity and quality of the notice and explanations should be made public whenever possible.

5.Ti=HUMAN ALTERNATIVES, CONSIDERATION, AND FALLBACK

5.sec=You should be able to opt out, where appropriate, and have access to a person who can quickly consider and remedy problems you encounter. You should be able to opt out from {_automated_systems} in favor of a human alternative, where appropriate. Appropriateness should be determined based on reasonable expectations in a given context and with a focus on ensuring broad accessibility and protecting the public from especially harmful impacts. In some cases, a human or other alternative may be required by law. You should have access to timely human consideration and remedy by a fallback and escalation process if an {_automated_system} fails, it produces an error, or you would like to appeal or contest its impacts on you. Human consideration and fallback should be accessible, {_equitable}, effective, maintained, accompanied by appropriate operator training, and should not impose an unreasonable burden on the public. Automated systems with an intended use within {_sensitive_domains}, including, but not limited to, criminal justice, employment, education, and health, should additionally be tailored to the purpose, provide meaningful access for oversight, include training for any people interacting with the system, and incorporate human consideration for adverse or high-risk decisions. Reporting that includes a description of these human governance processes and assessment of their timeliness, accessibility, outcomes, and effectiveness should be made public whenever possible.


00.sec=Definitions for key terms in The Blueprint for an AI Bill of Rights can be found in Applying the Blueprint for an AI Bill of Rights. Accompanying analysis and tools for actualizing each principle can be found in the Technical Companion.


=[G/Z/ol/5]

r00t=<table><tr><td>{Sec}</td><td>{Def.Sec}</td></tr></table>

Note=Applying The Blueprint for an AI Bill of Rights

Def.Ti=DEFINITIONS

Def.algorithmic_discrimination.Ti=ALGORITHMIC DISCRIMINATION

Def.algorithmic_discrimination.sec=“{DefT.algorithmic_discrimination}” occurs when {_automated_systems} contribute to unjustified different treatment or impacts disfavoring people based on their race, color, ethnicity, sex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexual orientation), religion, age, national origin, disability, veteran status, genetic information, or any other classification protected by law. Depending on the specific circumstances, such algorithmic discrimination may violate legal protections. Throughout this framework the term “algorithmic discrimination” takes this meaning (and not a technical understanding of discrimination as distinguishing between items).

Def.algorithmic_discrimination.=[G/Z/Base]

Def.automated_system.Ti=AUTOMATED SYSTEM

Def.automated_system.sec=An “{DefT.automated_system}” is any system, software, or process that uses computation as whole or part of a system to determine outcomes, make or aid decisions, inform policy implementation, collect data or observations, or otherwise interact with individuals and/or {_communities}. Automated systems include, but are not limited to, systems derived from machine learning, statistics, or other data processing or artificial intelligence techniques, and exclude {_passive_computing_infrastructure}. “{DefT.passive_computing_infrastructure}” is any intermediary technology that does not influence or determine the outcome of decision, make or aid in decisions, inform policy implementation, or collect data or observations, including web hosting, domain registration, networking, caching, data storage, or cybersecurity. Throughout this framework, {_automated_systems} that are considered in scope are only those that have the potential to meaningfully impact individuals’ or {_communities}’ {_rights,_opportunities,_or_access}.

Def.automated_system.=[G/Z/Base]

Def.communities.Ti=COMMUNITIES

Def.communities.sec=“{DefT.communities}” include: neighborhoods; social network connections (both online and offline); families (construed broadly); people connected by affinity, identity, or shared traits; and formal organizational ties. This includes Tribes, Clans, Bands, Rancherias, Villages, and other Indigenous {_communities}. AI and other data-driven {_automated_systems} most directly collect data on, make inferences about, and may cause harm to individuals. But the overall magnitude of their impacts may be most readily visible at the level of {_communities}. Accordingly, the concept of {_community} is integral to the scope of the Blueprint for an AI Bill of Rights. United States law and policy have long employed approaches for protecting the rights of individuals, but existing frameworks have sometimes struggled to provide protections when effects manifest most clearly at a {_community} level. For these reasons, the Blueprint for an AI Bill of Rights asserts that the harms of {_automated_systems} should be evaluated, protected against, and redressed at both the individual and {_community} levels.

Def.communities.=[G/Z/Base]

Def.equity.Ti=EQUITY

Def.equity.sec=“{DefT.equity}” means the consistent and systematic fair, just, and impartial treatment of all individuals. Systemic, fair, and just treatment must take into account the status of individuals who belong to {_underserved_communities} that have been denied such treatment, such as Black, Latino, and Indigenous and Native American persons, Asian Americans and Pacific Islanders and other persons of color; members of religious minorities; women, girls, and non-binary people; lesbian, gay, bisexual, transgender, queer, and intersex (LGBTQI+) persons; older adults; persons with disabilities; persons who live in rural areas; and persons otherwise adversely affected by persistent poverty or inequality.

Def.equity.=[G/Z/Base]

Def.rights,_opportunities,_or_access.Ti=RIGHTS, OPPORTUNITIES, OR ACCESS

Def.rights,_opportunities,_or_access.sec=“{DefT.rights,_opportunities,_or_access}” is used to indicate the scoping of this framework. It describes the set of: civil rights, civil liberties, and privacy, including freedom of speech, voting, and protections from discrimination, excessive punishment, unlawful surveillance, and violations of privacy and other freedoms in both public and private sector contexts; equal opportunities, including {_equitable} access to education, housing, credit, employment, and other programs; or, access to critical resources or services, such as healthcare, financial services, safety, social services, non-deceptive information about goods and services, and government benefits.

Def.rights,_opportunities,_or_access.=[G/Z/Base]

Def.sensitive_data.Ti=SENSITIVE DATA 

Note=In the original text, the definition of {_sensitive_data} differs in structure from the other definitions because it does not begin with the defined term in quotes. HazardJ slipped it in the section so that a link to the definition would work.

Def.sensitive_data.sec=Data and metadata are sensitive if they pertain to an individual in a {_sensitive_domain} (defined below); are generated by technologies used in a {_sensitive_domain}; can be used to infer data from a {_sensitive_domain} or {DefT.sensitive_data} about an individual (such as disability-related data, genomic data, biometric data, behavioral data, geolocation data, data related to interaction with the criminal justice system, relationship history and legal status such as custody and divorce information, and home, work, or school environmental data); or have the reasonable potential to be used in ways that are likely to expose individuals to meaningful harm, such as a loss of privacy or financial harm due to identity theft. Data and metadata generated by or about those who are not yet legal adults is also sensitive, even if not related to a {_sensitive_domain}. Such data includes, but is not limited to, numerical, text, image, audio, or video data. 

Def.sensitive_data.=[G/Z/Base]

Def.sensitive_domains.Ti=SENSITIVE DOMAINS

Def.sensitive_domains.sec=“{DefT.sensitive_domains}” are those in which activities being conducted can cause material harms, including significant adverse effects on human rights such as autonomy and dignity, as well as civil liberties and civil rights. Domains that have historically been singled out as deserving of enhanced data protections or where such enhanced protections are reasonably expected by the public include, but are not limited to, health, family planning and care, employment, education, criminal justice, and personal finance. In the context of this framework, such domains are considered sensitive whether or not the specifics of a system context would necessitate coverage under existing law, and domains and data that are considered sensitive are understood to change over time based on societal norms and context.

Def.sensitive_domains.=[G/Z/Base]

Def.surveillance_technology.Ti=SURVEILLANCE TECHNOLOGY

Def.surveillance_technology.sec=“{DefT.surveillance_technology}” refers to products or services marketed for or that can be lawfully used to detect, monitor, intercept, collect, exploit, preserve, protect, transmit, and/or retain data, identifying information, or communications concerning individuals or groups. This framework limits its focus to both government and commercial use of surveillance technologies when juxtaposed with real-time or subsequent automated analysis and when such systems have a potential for meaningful impact on individuals’ or {_communities}’ {_rights,_opportunities,_or_access}.

Def.surveillance_technology.=[G/Z/Base]

Def.underserved_communities.Ti=UNDERSERVED COMMUNITIES

Def.underserved_communities.sec=The term “{DefT.underserved_communities}” refers to communities that have been systematically denied a full opportunity to participate in aspects of economic, social, and civic life, as exemplified by the list in the preceding definition of “{_equity}.”

Def.underserved_communities.=[G/Z/Base]

Def.sec=<ol><li>{Def.automated_system.Sec}</li><li>{Def.communities.Sec}</li><li>{Def.equity.Sec}</li><li>{Def.rights,_opportunities,_or_access.Sec}</li><li>{Def.sensitive_data.Sec}</li><li>{Def.sensitive_domains.Sec}</li><li>{Def.surveillance_technology.Sec}</li><li>{Def.underserved_communities.Sec}</li></ol>

Def.=[G/Z/Base]



DefT.automated_system={_automated_system}

DefT.passive_computing_infrastructure={_Passive_computing_infrastructure}

DefT.communities={_Communities}

DefT.equity={_Equity}

DefT.rights,_opportunities,_or_access={_Rights,_opportunities,_or_access}

DefT.sensitive_data={_sensitive_data}

DefT.sensitive_domains={_Sensitive_domains}

DefT.surveillance_technology={_Surveillance_technology}

DefT.underserved_communities={_underserved_communities}

_automated_system=<a class='definedterm' href='{!!!}DefT.automated_system'>automated system</a>

_Passive_computing_infrastructure=<a class='definedterm' href='{!!!}DefT.passive_computing_infrastructure'>Passive computing infrastructure</a>

_Communities=<a class='definedterm' href='{!!!}DefT.communities'>Communities</a>

_Equity=<a class='definedterm' href='{!!!}DefT.equity'>Equity</a>

_Rights,_opportunities,_or_access=<a class='definedterm' href='{!!!}DefT.rights,_opportunities,_or_access'>Rights, opportunities, or access</a>

_sensitive_data=<a class='definedterm' href='{!!!}DefT.sensitive_data'>sensitive data</a>

_Sensitive_domains=<a class='definedterm' href='{!!!}DefT.sensitive_domains'>Sensitive domains</a>

_Surveillance_technology=<a class='definedterm' href='{!!!}DefT.surveillance_technology'>Surveillance technology</a>

_underserved_communities=<a class='definedterm' href='{!!!}DefT.underserved_communities'>underserved communities</a>

_equity=<a class='definedterm' href='{!!!}DefT.equity'>equity</a>

_equitable=<a class='definedterm' href='{!!!}DefT.equity'>equitable</a>

_communities=<a class='definedterm' href='{!!!}DefT.communities'>communities</a>

_community=<a class='definedterm' href='{!!!}DefT.communities'>community</a>

_automated_systems=<a class='definedterm' href='{!!!}DefT.automated_system'>automated systems</a>

_sensitive_domains=<a class='definedterm' href='{!!!}DefT.sensitive_domains'>sensitive domains</a>

_passive_computing_infrastructure=<a class='definedterm' href='{!!!}DefT.passive_computing_infrastructure'>passive computing infrastructure</a>

_sensitive_domain=<a class='definedterm' href='{!!!}DefT.sensitive_domains'>sensitive domain</a>

_rights,_opportunities,_or_access=<a class='definedterm' href='{!!!}DefT.rights,_opportunities,_or_access'>rights, opportunities, or access</a>

_surveillance_technologies=<a class='definedterm' href='{!!!}DefT.surveillance_technology'>surveillance technologies</a>

!!!=#